{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25728e80",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network & Classification\n",
    "Yun Xing. 2023-5-14\n",
    "\n",
    "The objective is to detect the security breach by predicting suspicious access using an RNN model and the provided Logfile data.\n",
    "\n",
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3ae925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "#from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from keras.preprocessing import sequence # fixed later \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e41b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"timestamp\":1502738402847,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"timestamp\":1502738402849,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"timestamp\":1502738402852,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"timestamp\":1502738402852,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"timestamp\":1502738402853,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  {\"timestamp\":1502738402847,\"method\":\"post\",\"qu...  0\n",
       "1  {\"timestamp\":1502738402849,\"method\":\"post\",\"qu...  0\n",
       "2  {\"timestamp\":1502738402852,\"method\":\"post\",\"qu...  0\n",
       "3  {\"timestamp\":1502738402852,\"method\":\"post\",\"qu...  0\n",
       "4  {\"timestamp\":1502738402853,\"method\":\"post\",\"qu...  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) We will read the code in slightly differently than before: \n",
    "dataframe = pd.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764e000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c) convert to a numpy.ndarray type. \n",
    "dataset = dataframe.values\n",
    "\n",
    "#d) Check the shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d5cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Store all rows and the 0th index as the feature data: \n",
    "X = dataset[:,0]\n",
    "\n",
    "# f) Store all rows and index 1 as the target variable: \n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c883f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) clean up the predictors: removing features that are not valuable, such as timestamp and source. \n",
    "\n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf1dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) tokenize data, which just means vectorizing our text. \n",
    "#.    we will tokenize every character (thus char_level = True)\n",
    "\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbb1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) pad our data as each observation has a different length\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j) Create your train set to be 75% of the data and your test set to be 25%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_processed, Y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4eb41fb",
   "metadata": {},
   "source": [
    "### Model 1 - RNN\n",
    "The first model will be a pretty minimal RNN with only an embedding layer, simple RNN and Dense layer. The next model we will add a few more layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6bd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim = num_words, output_dim = 32, input_length = max_log_length))\n",
    "\n",
    "model.add(SimpleRNN(units=32, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "          \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0c056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1024, 32)          2016      \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7696e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 19s 152ms/step - loss: 0.6182 - accuracy: 0.6286 - val_loss: 0.6128 - val_accuracy: 0.6406\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 17s 145ms/step - loss: 0.5671 - accuracy: 0.6627 - val_loss: 0.5144 - val_accuracy: 0.6873\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 17s 143ms/step - loss: 0.2578 - accuracy: 0.8938 - val_loss: 0.0860 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feac0ee3b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.asarray(X_train).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "\n",
    "model.fit(X_train, Y_train, validation_split=0.25, epochs=3, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3971f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 2s 37ms/step - loss: 0.0880 - accuracy: 0.9776\n",
      "Test loss: 0.08795161545276642\n",
      "Test accuracy: 0.977591872215271\n"
     ]
    }
   ],
   "source": [
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69620952",
   "metadata": {},
   "source": [
    "### Model 2 - LSTM + Dropout Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f54cfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "\n",
    "model2.add(tf.keras.layers.Embedding(input_dim = num_words, output_dim = 32, input_length = max_log_length))\n",
    "\n",
    "model2.add(LSTM(units = 64, recurrent_dropout = 0.5))\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(units=1, activation='sigmoid'))\n",
    "          \n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "463aeca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1024, 32)          2016      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9203d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 90s 746ms/step - loss: 0.4316 - accuracy: 0.7865 - val_loss: 0.1234 - val_accuracy: 0.9677\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 91s 768ms/step - loss: 0.1257 - accuracy: 0.9626 - val_loss: 0.0801 - val_accuracy: 0.9789\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 89s 751ms/step - loss: 0.1409 - accuracy: 0.9641 - val_loss: 0.0926 - val_accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feaadcf26d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, validation_split=0.25, epochs=3, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8acc957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 7s 123ms/step - loss: 0.0979 - accuracy: 0.9755\n",
      "Test loss: 0.09789314866065979\n",
      "Test accuracy: 0.9755004644393921\n"
     ]
    }
   ],
   "source": [
    "test_loss2, test_acc2 = model2.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('Test loss:', test_loss2)\n",
    "print('Test accuracy:', test_acc2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bd3045c",
   "metadata": {},
   "source": [
    "### Model 3: Customized Recurrent Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4459fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "283e1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_log_length))\n",
    "model3.add(LSTM(units=64, recurrent_dropout=0.5, return_sequences=True))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(units=64, recurrent_dropout=0.5)) # Adding an additional LSTM\n",
    "model3.add(Dense(units=1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01f5fd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 1024, 32)          2016      \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 1024, 64)          24832     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024, 64)          0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,937\n",
      "Trainable params: 59,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04b1a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 190s 2s/step - loss: 0.4284 - accuracy: 0.7924 - val_loss: 0.1116 - val_accuracy: 0.9673\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 183s 2s/step - loss: 0.1822 - accuracy: 0.9432 - val_loss: 0.0960 - val_accuracy: 0.9749\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 201s 2s/step - loss: 0.1443 - accuracy: 0.9561 - val_loss: 0.0715 - val_accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea895d57c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train, Y_train, validation_split=0.25, epochs=3, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "560fd98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 284ms/step - loss: 0.0815 - accuracy: 0.9765\n",
      "Test loss: 0.08145732432603836\n",
      "Test accuracy: 0.9765461683273315\n"
     ]
    }
   ],
   "source": [
    "test_loss3, test_acc3 = model3.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('Test loss:', test_loss3)\n",
    "print('Test accuracy:', test_acc3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb081890",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "#### 1. Difference between the relu activation function and the sigmoid activation function:\n",
    "\n",
    "1. ReLU (Rectified Linear Unit) is an activation function that maps any number to 0 if it is negative, and otherwise maps it to itself. The ReLU function is very good for networks with many layers because it can prevent vanishing gradients when training deep networks. it allows the network to learn more complex relationships between the inputs and outputs. \n",
    "\n",
    "2. Sigmoid maps any number between 0 and 1, inclusive, to itself. It is useful at binary classification tasks, where the output should be a probability between 0 and 1. When the input is very negative, the output of the sigmoid function is close to 0, and when the input is very positive, the output is close to 1. However, when the input is around 0, the output is close to 0.5, meaning the sigmoid function can struggle to differentiate between inputs that are close to zero.\n",
    "\n",
    "\n",
    "#### 2. What one epoch actually is (epoch was a parameter used in the .fit() method):\n",
    "\n",
    "The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. During each epoch, the neural network will make a prediction on each training sample in the dataset, compare that prediction to the actual output, and then adjust its weights and biases accordingly in order to improve its prediction accuracy. \n",
    "\n",
    "too few epochs may result in an underfit model that does not generalize well to new data, while too many epochs may result in an overfit model that has memorized the training data and does not generalize well to new data. T\n",
    "\n",
    "\n",
    "#### 3. How dropout works:\n",
    "\n",
    "Dropout is a regularization technique for neural network models to prevent overfitting, where randomly selected neurons are ignored during training. They are “dropped out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "In training, each neuron in the layer that has dropout applied has a probability p of being \"dropped out,\" i.e. its output is set to zero. The value of p is a hyperparameter that can be tuned, but is typically set to 0.5. Importantly, the dropout is applied independently to each example in the training batch, and each time the model is trained on a new batch, a new set of neurons are randomly dropped out.\n",
    "\n",
    "In testing, the full network is used, but the weights of the neurons that were dropped out during training are scaled down by the dropout rate p. This is done to ensure that the expected value of the output is the same at test time as it was during training. For example, if during training a neuron had a dropout rate of 0.5, then at test time its weight would be multiplied by 0.5. This is known as the \"inverted dropout\" technique.\n",
    "\n",
    "#### 4. Why problems such as this are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?\n",
    "\n",
    "CNNs are commonly used in solving problems related to spatial data, such as images. RNNs are better suited to analyzing temporal, sequential data, such as text or videos. RNNs are able to capture the temporal dependencies and long-term patterns in sequential data by maintaining a memory of the previous inputs in a sequence. A CNN has a different architecture from an RNN. CNNs are \"feed-forward neural networks\" that use filters and pooling layers, whereas RNNs feed results back into the network.\n",
    "\n",
    "In this problem, RNN works better because it envolves text data. \n",
    "\n",
    "#### 5. In this problem, an RNN problem could be solved using LSTM:\n",
    "\n",
    "LSTM networks solve the problem of vanishing gradients in RNN. Each LSTM cell contains three gates: the input gate, the forget gate, and the output gate. In LSTMs, the presence of the forget gate, along with the additive property of the cell state gradients, enables the network to update the parameter in such a way that the different sub gradients do not necessarily agree and behave in a similar manner, making it less likely that all of the T gradients will vanish, or in other words, the series of functions does not converge to zero. And our gradients do not vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d509a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
